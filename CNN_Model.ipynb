{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wVJcpmn8zbYy",
        "kdvY0BlhTUOE"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Download Crop Faces"
      ],
      "metadata": {
        "id": "wVJcpmn8zbYy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiyfBgiDzGFA"
      },
      "outputs": [],
      "source": [
        "%cd /content/WIKI_CROP\n",
        "!wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf wiki_crop.tar"
      ],
      "metadata": {
        "id": "nd2I0GTSza8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Packages"
      ],
      "metadata": {
        "id": "kdvY0BlhTUOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misc. packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# Keras\n",
        "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, RMSprop, adam\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# scikit tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "\n",
        "# Image manipulation\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "jPWVJRlrzsdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gather Training Data"
      ],
      "metadata": {
        "id": "H1-HTnggTY1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return bin classification\n",
        "def classify(age):\n",
        "  if 0 <= age < 20:\n",
        "    return 0\n",
        "  elif 20 <= age < 40:\n",
        "    return 1\n",
        "  elif 40 <= age < 60:\n",
        "    return 2\n",
        "  elif 60 <= age < 80:\n",
        "    return 3\n",
        "  elif 80 <= age <= 100:\n",
        "    return 4\n",
        "\n",
        "\n",
        "\n",
        "# Key info.\n",
        "training = []\n",
        "IMG_SIZE = 132\n",
        "\n",
        "# Load data\n",
        "annots = loadmat(\"/content/WIKI_CROP/wiki_crop/wiki.mat\")\n",
        "wiki = annots['wiki'][0][0]\n",
        "\n",
        "# Gather training data\n",
        "start = 0\n",
        "end = 4000\n",
        "for i in range(start, end):\n",
        "  # Resize images\n",
        "  filename = wiki[2][0][i][0]\n",
        "  img = cv2.imread(\"/content/WIKI_CROP/wiki_crop/\" + filename)\n",
        "  new_array = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "  # Real Age\n",
        "  born = wiki[0][0][i]\n",
        "  pic_time = wiki[1][0][i]\n",
        "  real_age = pic_time - int(born / 365)\n",
        "  classification = classify(real_age)\n",
        "\n",
        "  # Feed valid data\n",
        "  if classification is not None and real_age >= 0:\n",
        "    training.append([new_array, classification])\n",
        "\n",
        "random.shuffle(training)\n",
        "\n",
        "# \"Shape\" of CNN\n",
        "X =[]\n",
        "y =[]\n",
        "for features, label in training:\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Normalize & Categorize\n",
        "X = X.astype('float32')\n",
        "X /= 255\n",
        "Y = np_utils.to_categorical(y, 5)\n",
        "\n",
        "# 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)\n",
        "\n"
      ],
      "metadata": {
        "id": "37zK4nYW23n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile & Train"
      ],
      "metadata": {
        "id": "O62fhj90UhfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 500\n",
        "nb_epochs = 5\n",
        "nb_classes = 5\n",
        "\n",
        "kernel_size = 3\n",
        "nb_filters = 32\n",
        "pool_size = 2\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Conv2D(nb_filters * 2, (kernel_size, kernel_size), padding='same', activation=tf.nn.relu,\n",
        "                           input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides = 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(nb_filters, (kernel_size * 3, kernel_size * 3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides = 2),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(6,  activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(np.array(X_train), np.array(y_train), batch_size = batch_size, epochs = nb_epochs, verbose =1, validation_data = (np.array(X_test), np.array(y_test)))\n",
        "\n",
        "# Accuracy\n",
        "score = model.evaluate(np.array(X_test), np.array(y_test), verbose = 0)\n",
        "print(\"Accuracy: \" + str(score[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCoIzItQUkTB",
        "outputId": "0d2074d6-8c8d-41a4-9dec-69469d7cb321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 [==============================] - 35s 2s/step - loss: 2.5610 - accuracy: 0.4134 - val_loss: 1.7288 - val_accuracy: 0.6137\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 4s 577ms/step - loss: 1.5567 - accuracy: 0.5995 - val_loss: 1.3822 - val_accuracy: 0.6137\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 4s 559ms/step - loss: 1.2490 - accuracy: 0.5724 - val_loss: 1.1900 - val_accuracy: 0.4832\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 4s 579ms/step - loss: 1.1692 - accuracy: 0.5743 - val_loss: 1.1296 - val_accuracy: 0.6137\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 4s 564ms/step - loss: 1.1543 - accuracy: 0.5995 - val_loss: 1.1147 - val_accuracy: 0.6137\n",
            "Accuracy: 0.6136950850486755\n"
          ]
        }
      ]
    }
  ]
}